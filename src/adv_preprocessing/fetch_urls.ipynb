{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-19T13:23:55.020350Z",
     "start_time": "2025-02-19T13:23:54.964333Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from googlesearch import search\n",
    "import time\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:23:55.725090Z",
     "start_time": "2025-02-19T13:23:55.550077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_dir = os.getcwd()\n",
    "output_dir = os.path.join(base_dir, \"..\", \"..\", \"output\")\n",
    "\n",
    "file_path1 = os.path.join(output_dir, \"data_for_searching_news - Copy.xlsx\")\n",
    "df1 = pd.read_excel(file_path1)"
   ],
   "id": "eae7c2310d72824",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:23:56.306983Z",
     "start_time": "2025-02-19T13:23:56.285292Z"
    }
   },
   "cell_type": "code",
   "source": "len(df1)",
   "id": "aca81b602e0596bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:23:57.032630Z",
     "start_time": "2025-02-19T13:23:57.011476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to get news URLs using Google Search\n",
    "def get_news_urls(query, date):\n",
    "    formatted_date = pd.to_datetime(date).strftime(\"%Y-%m-%d\")\n",
    "    search_query = f\"{query} after:{formatted_date} before:{formatted_date} site:news\"\n",
    "\n",
    "    urls = []\n",
    "    try:\n",
    "        for url in search(search_query, num_results=2, lang=\"en\"):  # Fetch top 2 news URLs\n",
    "            urls.append(url)\n",
    "            time.sleep(10)  # Prevent blocking\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news for {query}: {e}\")\n",
    "\n",
    "    return urls"
   ],
   "id": "43cf061bbf9c2d99",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:23:57.640755Z",
     "start_time": "2025-02-19T13:23:57.634790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 76\n",
    "num_batches = len(df1) // batch_size + (1 if len(df1) % batch_size != 0 else 0)"
   ],
   "id": "fcda293e4da954b1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:37:12.910837Z",
     "start_time": "2025-02-19T13:23:58.421644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(num_batches):\n",
    "    print(f\"Processing batch {i + 1}/{num_batches}...\")\n",
    "\n",
    "    # Select batch\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(df1))\n",
    "    df_batch = df1.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "    # Apply function to batch\n",
    "    tqdm.pandas()\n",
    "    df_batch[\"news_urls\"] = df_batch.progress_apply(lambda row: get_news_urls(row[\"Query_Text\"], row[\"Date\"]), axis=1)\n",
    "\n",
    "    # Save each batch separately\n",
    "    file_path = os.path.join(output_dir, f\"news_batch_{i + 1}.csv\")\n",
    "    df_batch.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"Saved batch {i + 1} to {file_path}\")\n",
    "\n",
    "    # Wait for 1 minute before processing the next batch (except after last batch)\n",
    "    if i < num_batches - 1:\n",
    "        print(\"Waiting for 1 minute to avoid rate limiting...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "print(\"✅ All batches processed and saved!\")"
   ],
   "id": "7207f4809b5086b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [13:14<00:00, 21.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1 to C:\\Users\\Hp\\Desktop\\Data Science\\My Projects\\FakeNewsDetection\\src\\advanced\\..\\..\\output\\news_batch_1.csv\n",
      "✅ All batches processed and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a2892db35e8c714"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
